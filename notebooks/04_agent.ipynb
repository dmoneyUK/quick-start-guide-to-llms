{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aV1vLJFt_yyH"
      },
      "source": [
        "# Building an Agent with Generative Models and Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wia7kww9-hfd"
      },
      "outputs": [],
      "source": [
        "!pip install openai pinecone-client google-search-results alpaca-trade-api supabase diffusers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GYafT6u-hff"
      },
      "outputs": [],
      "source": [
        "from supabase import create_client, Client\n",
        "import datetime\n",
        "import re\n",
        "from copy import copy\n",
        "from functools import lru_cache\n",
        "\n",
        "import sys\n",
        "from io import StringIO\n",
        "from typing import Dict, Optional, Any\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "import os\n",
        "from diffusers import StableDiffusionPipeline, DiffusionPipeline\n",
        "import torch\n",
        "\n",
        "from serpapi import GoogleSearch\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Dict, Tuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfCiJh-x-hfg"
      },
      "outputs": [],
      "source": [
        "\n",
        "url: str = userdata.get('SUPABASE_URL')\n",
        "key: str = userdata.get('SUPABASE_API_KEY')\n",
        "supabase: Client = create_client(url, key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q966iIep-hfg"
      },
      "outputs": [],
      "source": [
        "class ToolInterface(BaseModel):\n",
        "    name: str\n",
        "    description: str\n",
        "\n",
        "    def use(self, input_text: str) -> str:\n",
        "        raise NotImplementedError(\"use() method not implemented\")  # Must implement in subclass\n",
        "\n",
        "class PythonREPLTool(ToolInterface):\n",
        "    \"\"\"A tool for running python code in a REPL.\"\"\"\n",
        "\n",
        "    globals: Optional[Dict] = Field(default_factory=dict, alias=\"_globals\")\n",
        "    locals: Optional[Dict] = Field(default_factory=dict, alias=\"_locals\")\n",
        "\n",
        "    name: str = \"Python REPL\"\n",
        "    description: str = (\n",
        "        \"A Python shell. Use this to execute python commands. \"\n",
        "        \"Input should be a valid python command. \"\n",
        "        \"If you want to see the output of a value, you should print it out \"\n",
        "        \"with `print(...)`. Include examples of using the code and print \"\n",
        "        \"the output.\"\n",
        "    )\n",
        "\n",
        "    def run(self, command: str) -> str:\n",
        "        \"\"\"Run command with own globals/locals and returns anything printed.\"\"\"\n",
        "        old_stdout = sys.stdout\n",
        "        sys.stdout = mystdout = StringIO()\n",
        "        try:\n",
        "            exec(command, self.globals, self.locals)\n",
        "            sys.stdout = old_stdout\n",
        "            output = mystdout.getvalue()\n",
        "        except Exception as e:\n",
        "            sys.stdout = old_stdout\n",
        "            output = str(e)\n",
        "        return output\n",
        "\n",
        "    def use(self, input_text: str) -> str:\n",
        "        input_text = input_text.strip().replace(\"```python\" , \"\")\n",
        "        input_text = input_text.strip().strip(\"```\")\n",
        "        return self.run(input_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppcvBAi1-hfh"
      },
      "outputs": [],
      "source": [
        "repl_tool = PythonREPLTool()\n",
        "result = repl_tool.use('print(1+1)')\n",
        "print(result)\n",
        "assert result == \"2\\n\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPM3BoEQ-hfh"
      },
      "outputs": [],
      "source": [
        "client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
        "\n",
        "class ChatLLM(BaseModel):\n",
        "    model: str = 'gpt-4o'\n",
        "    temperature: float = 0.0\n",
        "\n",
        "    def generate(self, prompt: str, stop: List[str] = None):\n",
        "        response = client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=self.temperature,\n",
        "            stop=stop\n",
        "        )\n",
        "        supabase.table('cost_projecting').insert({\n",
        "            'prompt': prompt,\n",
        "            'response': response.choices[0].message.content,\n",
        "            'input_tokens': response.usage.prompt_tokens,\n",
        "            'output_tokens': response.usage.completion_tokens,\n",
        "            'model': self.model,\n",
        "            'inference_params' : {\n",
        "                'temperature': self.temperature,\n",
        "                'stop': stop\n",
        "            },\n",
        "            'is_openai': True,\n",
        "            'app': 'AGENT'\n",
        "        }).execute()\n",
        "        return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qxjgkae9-hfi"
      },
      "outputs": [],
      "source": [
        "llm = ChatLLM()\n",
        "result = llm.generate(prompt='Who is the president of Turkey?')\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGhRvP12-hfi"
      },
      "outputs": [],
      "source": [
        "import PIL\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDqHNg7j-hfj"
      },
      "outputs": [],
      "source": [
        "FINAL_ANSWER_TOKEN = \"Assistant Response:\"\n",
        "OBSERVATION_TOKEN = \"Observation:\"\n",
        "THOUGHT_TOKEN = \"Thought:\"\n",
        "PROMPT_TEMPLATE = \"\"\"Today is {today} and you can use tools to get new information. Respond to the user's input as best as you can using the following tools:\n",
        "\n",
        "{tool_description}\n",
        "\n",
        "You must follow the following format for every single turn of the conversation:\n",
        "\n",
        "User Input: the input question you must answer\n",
        "Thought: comment on what you want to do next.\n",
        "Action: the action to take, exactly one element of [{tool_names}]\n",
        "Action Input: the input to the action\n",
        "Observation: the result of the action\n",
        "Thought: Now comment on what you want to do next.\n",
        "Action: the next action to take, exactly one element of [{tool_names}]\n",
        "Action Input: the input to the next action\n",
        "Observation: the result of the next action\n",
        "Thought: Now comment on what you want to do next.\n",
        "... (this Thought/Action/Action Input/Observation repeats until you are sure of the answer)\n",
        "Assistant Thought: I have enough information to respond to the user's input.\n",
        "Assistant Response: your final answer to the original input question\n",
        "User Input: the input question you must answer\n",
        "Thought: comment on what you want to do next.\n",
        "Action: the next action to take, exactly one element of [{tool_names}]\n",
        "Action Input: the input to the next action\n",
        "Observation: the result of the next action\n",
        "... (this Thought/Action/Action Input/Observation repeats until you are sure of the answer)\n",
        "Assistant Thought: I have enough information to respond to the user's input.\n",
        "Assistant Response: your final answer to the original input question\n",
        "\n",
        "You must end every round with \"Assistant Thought:\" and \"Assistant Response:\"\n",
        "\n",
        "Begin:\n",
        "\n",
        "{previous_responses}\n",
        "\"\"\"\n",
        "\n",
        "class Agent(BaseModel):\n",
        "    llm: ChatLLM\n",
        "    tools: List[ToolInterface]\n",
        "    prompt_template: str = PROMPT_TEMPLATE\n",
        "    max_loops: int = 5\n",
        "    # The stop pattern is used, so the LLM does not hallucinate until the end\n",
        "    stop_pattern: List[str] = [f'\\n{OBSERVATION_TOKEN}', f'\\n\\t{OBSERVATION_TOKEN}']\n",
        "    human_responses: List[str] = []\n",
        "    ai_responses: List[str] = []\n",
        "    pretty_responses: List[str] = []\n",
        "    verbose: bool = False\n",
        "\n",
        "    @property\n",
        "    def tool_description(self) -> str:\n",
        "        return \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])\n",
        "\n",
        "    @property\n",
        "    def tool_names(self) -> str:\n",
        "        return \", \".join([tool.name for tool in self.tools])\n",
        "\n",
        "    @property\n",
        "    def tool_by_names(self) -> Dict[str, ToolInterface]:\n",
        "        return {tool.name: tool for tool in self.tools}\n",
        "\n",
        "    def run(self, question: str):\n",
        "        self.ai_responses.append(f'User Input: {question}')\n",
        "        self.human_responses.append(question)\n",
        "        previous_responses = copy(self.ai_responses)\n",
        "        num_loops = 0\n",
        "        prompt = self.prompt_template.format(\n",
        "                today = datetime.date.today(),\n",
        "                tool_description=self.tool_description,\n",
        "                tool_names=self.tool_names,\n",
        "                question=question,\n",
        "                previous_responses='{previous_responses}'\n",
        "        )\n",
        "        if self.verbose:\n",
        "            print('------')\n",
        "            print(prompt.format(previous_responses=''))\n",
        "            print('------')\n",
        "        while num_loops < self.max_loops:\n",
        "            num_loops += 1\n",
        "            curr_prompt = prompt.format(previous_responses='\\n'.join(previous_responses))\n",
        "            generated, tool, tool_input = self.decide_next_action(curr_prompt)\n",
        "            if self.verbose:\n",
        "                print('------')\n",
        "                print('CURR PROMPT')\n",
        "                print('------')\n",
        "                print(curr_prompt)\n",
        "                print('------')\n",
        "                print('------')\n",
        "                print('RAW GENERATED')\n",
        "                print('------')\n",
        "                print(generated)\n",
        "                print('------')\n",
        "            if tool == 'Assistant Response':\n",
        "                if self.verbose:\n",
        "                    print('------')\n",
        "                    print('FINAL PROMPT')\n",
        "                    print('------')\n",
        "                    print(curr_prompt)\n",
        "                    print('------')\n",
        "                self.ai_responses.append(f'Assistant Response: {tool_input}')\n",
        "                return tool_input\n",
        "            if tool not in self.tool_by_names:\n",
        "                raise ValueError(f\"Unknown tool: {tool}\")\n",
        "            if self.verbose:\n",
        "                print('tool_input', tool_input)\n",
        "            tool_result = self.tool_by_names[tool].use(tool_input)\n",
        "            if type(tool_result) == PIL.Image.Image:\n",
        "                plt.imshow(tool_result)\n",
        "                plt.show()\n",
        "            generated += f\"\\n{OBSERVATION_TOKEN} {tool_result}\\n\"\n",
        "            self.ai_responses.append(generated.strip())\n",
        "            if self.verbose:\n",
        "                print('------')\n",
        "                print('PARSED GENERATED')\n",
        "                print('------')\n",
        "                print(generated)\n",
        "                print('------')\n",
        "            previous_responses.append(generated)\n",
        "\n",
        "    def decide_next_action(self, prompt: str) -> str:\n",
        "        generated = self.llm.generate(prompt, stop=self.stop_pattern)\n",
        "\n",
        "        tool, tool_input = self._parse(generated)\n",
        "        return generated, tool, tool_input\n",
        "\n",
        "    def _parse(self, generated: str) -> Tuple[str, str]:\n",
        "        if FINAL_ANSWER_TOKEN in generated:\n",
        "            if self.verbose:\n",
        "                print('------')\n",
        "                print('FINAL ANSWER')\n",
        "                print('------')\n",
        "                print(generated)\n",
        "                print('------')\n",
        "            final_answer = generated.split(FINAL_ANSWER_TOKEN)[-1].strip()\n",
        "            self.pretty_responses.append(final_answer)\n",
        "            return \"Assistant Response\", final_answer\n",
        "        regex = r\"Action: [\\[]?(.*?)[\\]]?[\\n]*Action Input:[\\s]*(.*)\"\n",
        "        match = re.search(regex, generated, re.DOTALL)\n",
        "        if not match:\n",
        "            raise ValueError(f\"Output of LLM is not parsable for next tool use: `{generated}`\")\n",
        "        tool = match.group(1).strip()\n",
        "        tool_input = match.group(2)\n",
        "        return tool, tool_input.strip(\" \").strip('\"')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nk9c2hG9-hfj"
      },
      "outputs": [],
      "source": [
        "agent = Agent(llm=ChatLLM(), tools=[PythonREPLTool()])\n",
        "result = agent.run(\"please write me a function to take in a number and return 2 times it\")\n",
        "\n",
        "print(f\"Final answer is {result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMqARLb1-hfj"
      },
      "outputs": [],
      "source": [
        "agent.pretty_responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHyki0-F-hfk"
      },
      "outputs": [],
      "source": [
        "for a in agent.ai_responses:\n",
        "    print(a)\n",
        "    print('------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NebQwPwF-hfk"
      },
      "outputs": [],
      "source": [
        "agent = Agent(llm=ChatLLM(), tools=[PythonREPLTool()])\n",
        "\n",
        "result = agent.run(\"Who was the first US President?\")\n",
        "\n",
        "print(f\"Final answer is {result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UODVw2aD-hfk"
      },
      "outputs": [],
      "source": [
        "# decided it needed python (incorrectly)\n",
        "for a in agent.ai_responses:\n",
        "    print(a)\n",
        "    print('------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzTfh9la-hfk"
      },
      "outputs": [],
      "source": [
        "class SimplyRespond(ToolInterface):  # sometimes referred to as \"directly answer\"\n",
        "\n",
        "    name: str = 'Simply Respond'\n",
        "    description: str = 'Choose this option if the user is giving a pleasantry '\n",
        "    'or if the answer is answerable from historical knowledge like a Q/A. The action input is the answer to the query.'\n",
        "\n",
        "    def use(self, input_text: str) -> str:\n",
        "        return input_text\n",
        "\n",
        "class Inquire(ToolInterface):\n",
        "\n",
        "    name: str = 'Inquire for more Information'\n",
        "    description: str = 'Choose this option if further information is required '\n",
        "    'to respond to the user. The action input is a question to ask the user'\n",
        "\n",
        "    def use(self, input_text: str) -> str:\n",
        "        return input_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olBIzquo-hfk"
      },
      "outputs": [],
      "source": [
        "agent = Agent(llm=ChatLLM(), tools=[PythonREPLTool(), SimplyRespond(), Inquire()])\n",
        "\n",
        "result = agent.run(\"Who was the first US President?\")\n",
        "\n",
        "print(f\"Final answer is {result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkDtHfj1-hfk"
      },
      "outputs": [],
      "source": [
        "# decided it needed python (incorrectly)\n",
        "for a in agent.ai_responses:\n",
        "    print(a)\n",
        "    print('------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUJspF6_-hfk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZI0GfLez-hfk"
      },
      "outputs": [],
      "source": [
        "@lru_cache(maxsize=None)\n",
        "def search(query: str) -> str:\n",
        "    params: dict = {\n",
        "        \"engine\": \"google\",\n",
        "        \"google_domain\": \"google.com\",\n",
        "        \"gl\": \"us\",\n",
        "        \"hl\": \"en\",\n",
        "        \"q\": query,\n",
        "        \"api_key\": userdata.get(\"SERP_API_KEY\"),\n",
        "    }\n",
        "\n",
        "    with HiddenPrints():\n",
        "        search = GoogleSearch(params)\n",
        "        res = search.get_dict()\n",
        "\n",
        "    return _process_response(res)\n",
        "\n",
        "\n",
        "def _process_response(res: dict) -> str:\n",
        "    \"\"\"Process response from SerpAPI.\"\"\"\n",
        "    if \"error\" in res.keys():\n",
        "        raise ValueError(f\"Got error from SerpAPI: {res['error']}\")\n",
        "    if \"answer_box\" in res.keys() and \"answer\" in res[\"answer_box\"].keys():\n",
        "        toret = res[\"answer_box\"][\"answer\"]\n",
        "    elif \"answer_box\" in res.keys() and \"snippet\" in res[\"answer_box\"].keys():\n",
        "        toret = res[\"answer_box\"][\"snippet\"]\n",
        "    elif (\n",
        "        \"answer_box\" in res.keys()\n",
        "        and \"snippet_highlighted_words\" in res[\"answer_box\"].keys()\n",
        "    ):\n",
        "        toret = res[\"answer_box\"][\"snippet_highlighted_words\"][0]\n",
        "    elif (\n",
        "        \"sports_results\" in res.keys()\n",
        "        and \"game_spotlight\" in res[\"sports_results\"].keys()\n",
        "    ):\n",
        "        toret = res[\"sports_results\"][\"game_spotlight\"]\n",
        "    elif (\n",
        "        \"knowledge_graph\" in res.keys()\n",
        "        and \"description\" in res[\"knowledge_graph\"].keys()\n",
        "    ):\n",
        "        toret = res[\"knowledge_graph\"][\"description\"]\n",
        "    elif \"snippet\" in res[\"organic_results\"][0].keys():\n",
        "        toret = res[\"organic_results\"][0][\"snippet\"]\n",
        "\n",
        "    else:\n",
        "        toret = \"No good search result found\"\n",
        "    return toret\n",
        "\n",
        "\n",
        "class HiddenPrints:\n",
        "    \"\"\"Context manager to hide prints.\"\"\"\n",
        "\n",
        "    def __enter__(self) -> None:\n",
        "        \"\"\"Open file to pipe stdout to.\"\"\"\n",
        "        self._original_stdout = sys.stdout\n",
        "        sys.stdout = open(os.devnull, \"w\")\n",
        "\n",
        "    def __exit__(self, *_: Any) -> None:\n",
        "        \"\"\"Close file that stdout was piped to.\"\"\"\n",
        "        sys.stdout.close()\n",
        "        sys.stdout = self._original_stdout\n",
        "\n",
        "\n",
        "\n",
        "class SerpAPITool(ToolInterface):\n",
        "    \"\"\"Tool for Google search results.\"\"\"\n",
        "\n",
        "    name: str = \"Google Search\"\n",
        "    description: str = \"Get specific information from a search query. Input should be a  question like 'How to add number in Clojure?'. Result will be the answer to the question.\"\n",
        "\n",
        "    def use(self, input_text: str) -> str:\n",
        "        return search(input_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOWeile5-hfl"
      },
      "outputs": [],
      "source": [
        "s = SerpAPITool()\n",
        "res = s.use(\"Who is the pope?\")\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxLGUoZe-hfl"
      },
      "outputs": [],
      "source": [
        "agent = Agent(llm=ChatLLM(), tools=[PythonREPLTool(), SerpAPITool()])\n",
        "result = agent.run(\"How many people are allowed on the baseball field during play?\")\n",
        "\n",
        "print(f\"Final answer is {result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Su_sMCa6-hfl"
      },
      "outputs": [],
      "source": [
        "for a in agent.ai_responses:\n",
        "    print(a)\n",
        "    print('---')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GScNKSAG-hfl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5XHx9Jc-hfl"
      },
      "outputs": [],
      "source": [
        "agent = Agent(llm=ChatLLM(), tools=[SimplyRespond(), PythonREPLTool(), SerpAPITool(),  Inquire()])\n",
        "result = agent.run(\"What is 1064 + the current price of ethereum in USD?\")\n",
        "\n",
        "print(f\"Final answer is {result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K23XNlY8-hfl"
      },
      "outputs": [],
      "source": [
        "agent.run(\"one more thing, what day of the week is tomorrow?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMfxKIC8-hfl"
      },
      "outputs": [],
      "source": [
        "agent.run(\"I forgot to ask, what is the reversed name of the current executive director of common crawl?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1lqUsuw-hfl"
      },
      "outputs": [],
      "source": [
        "for h, a in list(zip(agent.human_responses, agent.pretty_responses)):\n",
        "    print(f\"{h} -> \\n\\t{a}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJ19SyLm-hfl"
      },
      "outputs": [],
      "source": [
        "for a in agent.ai_responses:\n",
        "    print(a)\n",
        "    print('---')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfnWMLO0-hfl"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, Dict\n",
        "from pydantic import BaseModel\n",
        "import alpaca_trade_api as tradeapi\n",
        "\n",
        "class CheckStockBalance(ToolInterface):\n",
        "    api_key: str\n",
        "    api_secret: str\n",
        "    base_url: str\n",
        "    api_version: str = 'v2'\n",
        "    \"\"\"A tool for checking the current stock wallet balance using Alpaca API.\"\"\"\n",
        "\n",
        "    name: str = \"Check Stock Balance\"\n",
        "    description: str = (\n",
        "        \"A tool that uses the Alpaca Trade API to retrieve the current wallet balance, \"\n",
        "        \"allowing users to check their available cash and stock positions. \"\n",
        "        \"The action input to this tool is exactly one of the following commands: \"\n",
        "        \"[get_balance]\"\n",
        "\n",
        "    )\n",
        "\n",
        "    def get_account_balance(self) -> Dict[str, float]:\n",
        "        \"\"\"Retrieves the current wallet balance including cash and account value.\"\"\"\n",
        "        alpaca_api = tradeapi.REST(\n",
        "            self.api_key, self.api_secret, self.base_url,\n",
        "            api_version=self.api_version)\n",
        "        account = alpaca_api.get_account()\n",
        "        return {\n",
        "            \"cash\": float(account.cash),\n",
        "            \"portfolio_value\": float(account.portfolio_value)\n",
        "        }\n",
        "\n",
        "    def use(self, command: str) -> str:\n",
        "        \"\"\"Run a command to get the account balance.\"\"\"\n",
        "        if command == \"get_balance\":\n",
        "            balance = self.get_account_balance()\n",
        "            return f\"Cash: ${balance['cash']}, Portfolio Value: ${balance['portfolio_value']}\"\n",
        "        else:\n",
        "            return \"Unknown command. Please use 'get_balance' to check your wallet balance.\"\n",
        "\n",
        "check_stock_balance = CheckStockBalance(\n",
        "    api_key=userdata.get('PAPER_ALPACA_API_KEY'),\n",
        "    api_secret=userdata.get('PAPER_ALPACA_API_SECRET'),\n",
        "    base_url='https://paper-api.alpaca.markets'\n",
        "    )\n",
        "\n",
        "check_stock_balance.use('get_balance')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNbIutaE-hfm"
      },
      "outputs": [],
      "source": [
        "sawyer = Agent(llm=ChatLLM(), tools=[\n",
        "    check_stock_balance,\n",
        "    PythonREPLTool(),\n",
        "    SerpAPITool(),\n",
        "    SimplyRespond(),\n",
        "    Inquire()\n",
        "])\n",
        "\n",
        "def chat_with(agent):\n",
        "# Chat loop\n",
        "    while True:\n",
        "        user_input = input(\"You (exit to quit): \")\n",
        "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "            break\n",
        "        response = agent.run(user_input)\n",
        "        print(\"Agent:\", response)\n",
        "\n",
        "    # After exiting the loop, print all AI responses\n",
        "    print(\"\\nAI Responses:\")\n",
        "    for response in agent.ai_responses:\n",
        "        print(response)\n",
        "\n",
        "chat_with(agent := sawyer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZ9S_AbN-hfm"
      },
      "outputs": [],
      "source": [
        "pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\").to(\"cuda\")\n",
        "\n",
        "class StableDiffusionTool(ToolInterface):\n",
        "    \"\"\"A tool for generating images\"\"\"\n",
        "    name: str = \"Stable Diffusion\"\n",
        "    description: str = (\n",
        "        \"A tool for performing image generation using stable diffusion. \"\n",
        "        \"The action input to this tool is a prompt to create an image \"\n",
        "        \"like 'A photo of a cat on a white background' or 'A surrealist painting of a sunset'.\"\n",
        "    )\n",
        "\n",
        "    def use(self, prompt: str) -> str:\n",
        "        \"\"\"Run the stable diffusion tool.\"\"\"\n",
        "        return pipe(prompt).images[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkFtZnVP-hfm"
      },
      "outputs": [],
      "source": [
        "stable_diffusion_tool = StableDiffusionTool()\n",
        "stable_diffusion_tool.use('A photo of a black cat lounging around')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_G3SYBCn-hfm"
      },
      "outputs": [],
      "source": [
        "sawyer = Agent(llm=ChatLLM(), tools=[\n",
        "    c,\n",
        "    PythonREPLTool(),\n",
        "    SerpAPITool(),\n",
        "    SimplyRespond(),\n",
        "    Inquire(),\n",
        "    StableDiffusionTool()\n",
        "])\n",
        "\n",
        "chat_with(sawyer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHnlYrWa-hfm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8bY5e66-hfm"
      },
      "outputs": [],
      "source": [
        "# Bringing our RAG application back"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ap2oglOB-hfm"
      },
      "outputs": [],
      "source": [
        "from pinecone import Pinecone\n",
        "\n",
        "class LookupTool(ToolInterface):\n",
        "    \"\"\"A tool for performing semantic searches using Pinecone and OpenAI embeddings.\"\"\"\n",
        "\n",
        "    # Pinecone setup\n",
        "    pinecone_key: str = userdata.get('PINECONE_API_KEY')\n",
        "    INDEX_NAME: str = 'semantic-search-rag'\n",
        "    ENGINE: str = 'text-embedding-3-small'\n",
        "    NAMESPACE: str = 'default'\n",
        "\n",
        "    name: str = \"Semantic Search Tool\"\n",
        "    description: str = (\n",
        "        \"A tool for performing information lookup. Look something up if you \"\n",
        "        \"are being asked about a fact. Even if the retrieved information is \"\n",
        "        \"irrelevant, mention that in the thought and answer the question.\"\n",
        "    )\n",
        "\n",
        "    def __init__(self, **data):\n",
        "        super().__init__(**data)\n",
        "\n",
        "    def get_embeddings(self, texts, engine=ENGINE):\n",
        "        response = OpenAI(api_key=userdata.get('OPENAI_API_KEY')).embeddings.create(\n",
        "            input=texts,\n",
        "            model=engine\n",
        "        )\n",
        "        return [d.embedding for d in list(response.data)]\n",
        "\n",
        "    def get_embedding(self, text, engine=ENGINE):\n",
        "        return self.get_embeddings([text], engine)[0]\n",
        "\n",
        "    def my_hash(self, s):\n",
        "        return hashlib.md5(s.encode()).hexdigest()\n",
        "\n",
        "    def query_from_pinecone(self, query, top_k=3, include_metadata=True):\n",
        "        # get embedding from THE SAME embedder as the documents\n",
        "        query_embedding = self.get_embedding(query, engine=self.ENGINE)\n",
        "        index = Pinecone(api_key=userdata.get('PINECONE_API_KEY')).Index(name=self.INDEX_NAME)\n",
        "\n",
        "        return index.query(\n",
        "          vector=query_embedding,\n",
        "          top_k=top_k,\n",
        "          namespace=self.NAMESPACE,\n",
        "          include_metadata=include_metadata   # gets the metadata (dates, text, etc)\n",
        "        ).get('matches')[0]['metadata']['text']\n",
        "\n",
        "    def use(self, query):\n",
        "        return self.query_from_pinecone(query)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keOGTse3-hfm"
      },
      "outputs": [],
      "source": [
        "l = LookupTool()\n",
        "l.use('I lost my medicare card')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iP90Se49-hf2"
      },
      "outputs": [],
      "source": [
        "agent = Agent(llm=ChatLLM(), tools=[PythonREPLTool(), LookupTool()])\n",
        "result = agent.run(\"please write me a function to take in a number and return 2 times it\")\n",
        "\n",
        "print(f\"Final answer is {result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cs-0M_X2-hf2"
      },
      "outputs": [],
      "source": [
        "agent = Agent(llm=ChatLLM(), tools=[PythonREPLTool(), LookupTool()])\n",
        "result = agent.run(\"how to replace my medicare card\")\n",
        "\n",
        "print(f\"Final answer is {result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_7HnLVj-hf3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "response = supabase.table('cost_projecting').select(\"*\").eq('app', 'AGENT').execute()\n",
        "completions_df = pd.DataFrame(response.data)\n",
        "completions_df.index = pd.to_datetime(completions_df['created_at'])\n",
        "\n",
        "completions_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0sNipmN-hf3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}